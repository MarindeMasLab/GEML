{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aaa6785-f0ca-49d4-91bb-8b45883f59cb",
   "metadata": {},
   "source": [
    "# Integrating Machine Learning with Metabolic Models for Precision Trauma Care: Personalized ENDOTYPE Stratification and Metabolic Target Identification\n",
    "\n",
    "Authors:\n",
    "- Igor Marin de Mas (Copenhagen University Hospital, Rigshospitalet)\n",
    "- Lincoln Moura (Universidade Federal do Ceará)\n",
    "- Fernando Luiz Marcelo Antunes (Universidade Federal do Ceará)\n",
    "- Josep Maria Guerrero (Aalborg University)\n",
    "- Pär Ingemar Johansson (Copenhagen University Hospital, Rigshospitalet)\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This code is designed to preprocess and analyze datasets stored in MATLAB `.mat` files, with the goal of cleaning and transforming the data for further analysis. It performs several essential steps, including data validation, handling missing values, removing outliers, numerical stabilization, and dimensionality reduction using Principal Component Analysis (PCA). The preprocessed data is then saved in CSV format for easy access and further use.\n",
    "\n",
    "## Workflow Overview\n",
    "1. **Loading MATLAB files**: Extracting data and converting it into a tabular format (`DataFrame`) using Python libraries.\n",
    "2. **Preprocessing**: Cleaning the dataset by removing rows with problematic values (e.g., zeros), stabilizing numerical values, handling outliers, and replacing missing data with column means.\n",
    "3. **Dimensionality Reduction**: Utilizing PCA to reduce the number of features while retaining the majority of the dataset’s explainability (variance).\n",
    "4. **Saving Results**: Storing the transformed datasets and PCA explainability metrics in CSV files for later use.\n",
    "\n",
    "## Libraries Used\n",
    "This code leverages powerful Python libraries, including:\n",
    "- `pandas` for data manipulation and saving datasets in CSV format.\n",
    "- `scipy` for loading MATLAB `.mat` files.\n",
    "- `numpy` for numerical operations.\n",
    "- `scikit-learn` for data scaling (`StandardScaler`) and dimensionality reduction (`PCA`).\n",
    "- `os` and `glob` for navigating directories and handling files.\n",
    "\n",
    "## Key Features\n",
    "- **Batch Processing**: The code processes multiple directories and datasets simultaneously, making it efficient for large-scale data analysis.\n",
    "- **Data Cleaning**: It ensures the dataset’s quality by addressing missing values, outliers, and numerical stability.\n",
    "- **Dimensionality Reduction**: PCA is applied to simplify the dataset while retaining high explainability, making it suitable for machine learning models or statistical analysis.\n",
    "\n",
    "This implementation is ideal for preparing raw data for further computational analysis or machine learning workflows. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ad2b9c-db1c-40f4-ac36-d3aeb8bb0d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess(data_frame):\n",
    "    \"\"\"\n",
    "    Preprocess the input dataset by cleaning, handling outliers, ensuring numerical stability,\n",
    "    and applying PCA for dimensionality reduction.\n",
    "\n",
    "    Args:\n",
    "        data_frame (pd.DataFrame): Input dataset to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Processed dataset (pd.DataFrame) and explainability score (float).\n",
    "    \"\"\"\n",
    "    # Check if the input DataFrame is valid\n",
    "    if data_frame.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty. Please provide a valid dataset.\")\n",
    "\n",
    "    print(\"Shape before preprocess: \", data_frame.shape)\n",
    "\n",
    "    # Remove rows with specific indices where all values are zero\n",
    "    # The indices representing rows with all zero values were carefully analyzed and hardcoded to ensure their removal across all patient files.\n",
    "    removal_indices = [\n",
    "        7, 8, 9, 10, 11, 12, 13, 14, 20, 39, 67, 91, 143, 145, 146, 177, 189, 205, 240, 241, 242, \n",
    "        251, 298, 302, 335, 373, 383, 399, 460, 466, 467, 468, 480, 493, 510, 511, 519, 520, 536, \n",
    "        537, 538, 539, 540, 541, 542, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, \n",
    "        651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 666, 667, 668, 669, \n",
    "        670, 671, 672, 673, 676, 678, 679, 681, 682, 688, 690, 691, 692, 695, 697, 698, 699, 701, \n",
    "        702, 704, 706, 707, 708, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, \n",
    "        723, 724, 725, 726, 727, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, \n",
    "        744, 746, 747, 749, 750, 753, 755, 762, 763, 766, 768, 770, 772, 774, 775, 776, 777, 778, \n",
    "        779, 782, 783, 785, 790, 792, 793, 797, 798, 799, 800, 802, 804, 806, 807, 809, 810, 811, \n",
    "        814, 815, 816, 817, 818, 820, 823, 825, 826, 828, 829, 830, 831, 833, 836, 838, 840, 854, \n",
    "        868, 870, 871, 872, 873, 900, 903, 912, 1030, 1146, 1161, 1165, 1193, 1208, 1282, 1378, \n",
    "        1402, 1416, 1434, 1458, 1471, 1542, 1543, 1545, 1548, 1601, 1609, 1610, 1613, 1614, 1632, \n",
    "        1640, 1645, 1646, 1648, 1649, 1749, 1755, 1769, 1771, 1778, 1806, 1839, 1931, 2008, 2121, \n",
    "        2122, 2123, 2124, 2155, 2156, 2171, 2180, 2207, 2236, 2255, 2261, 2286, 2350, 2351, 2357, \n",
    "        2366, 2375, 2404, 2434, 2512, 2519, 2590, 2596, 2611, 2612, 2616, 2617, 2618, 2619, 2620, \n",
    "        2621, 2622, 2623, 2624, 2627, 2628, 2629, 2630, 2631, 2634, 2638, 2686, 2687, 2688, 2689, \n",
    "        2691, 2692, 2696, 2697, 2702, 2703, 2704, 2711, 2712, 2713, 2714, 2715, 2716, 2719, 2720, \n",
    "        2721, 2722, 2723, 2724, 2725, 2726, 2727, 2731, 2732, 2733, 2734, 2736, 2738, 2739, 2742, \n",
    "        2744, 2747, 2748, 2749, 2753, 2755, 2756, 2757, 2758, 2759, 2760, 2763, 2765, 2767, 2771, \n",
    "        2772, 2773, 2774, 2776, 2782, 2783, 2785, 2787, 2788, 2789, 2794, 2795, 2796, 2797, 2798, \n",
    "        2799, 2805, 2906, 2909, 2911, 2986, 2994, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, \n",
    "        3005\n",
    "    ]\n",
    "\n",
    "    # Check if rows with zeros match the specified indices\n",
    "    zero_indices = (data_frame[(data_frame == 0).any(axis=1)].index).tolist()\n",
    "    if zero_indices == removal_indices:\n",
    "        print(\"Number of reactions exactly equal\")\n",
    "        data_frame = data_frame[(data_frame != 0).any(axis=1)]\n",
    "    else:\n",
    "        raise Exception(\"Number of reactions with zero mismatch ERROR!\")\n",
    "\n",
    "    # Ensure numerical stability by applying a threshold\n",
    "    stabilize_values = lambda x: 0 if (x < 0.001 and x > -0.001) else x\n",
    "    data_frame = data_frame.applymap(stabilize_values)  # Apply the stability function\n",
    "    data_frame = data_frame.round(3)  # Round values to 3 decimal places\n",
    "\n",
    "    # Handle outliers by replacing them with the mean\n",
    "    first_quartile = data_frame.quantile(0.25)\n",
    "    third_quartile = data_frame.quantile(0.75)\n",
    "    interquartile_step = 1.5 * (third_quartile - first_quartile)\n",
    "    data_frame = data_frame[\n",
    "        (data_frame >= (first_quartile - interquartile_step)) &\n",
    "        (data_frame <= (third_quartile + interquartile_step))\n",
    "    ]\n",
    "    data_frame = data_frame.fillna(data_frame.mean())  # Replace NaN values with column means\n",
    "\n",
    "    # Apply PCA to reduce dimensionality while keeping explainability > 0.99\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA(n_components=600)\n",
    "    transformed_data = scaler.fit_transform(data_frame)\n",
    "    data_frame = pd.DataFrame(pca.fit_transform(transformed_data), index=data_frame.index)\n",
    "\n",
    "    # Display PCA explainability and final shape of the dataset\n",
    "    explained_variance = pca.explained_variance_ratio_.sum()\n",
    "    print(\"Explainability: \", explained_variance)\n",
    "    print(\"Shape after preprocess: \", data_frame.shape)\n",
    "\n",
    "    return data_frame, explained_variance\n",
    "\n",
    "\n",
    "# Function to load and convert MATLAB files to DataFrame\n",
    "def load_mat_file(file_path):\n",
    "    \"\"\"\n",
    "    Load a MATLAB .mat file and extract its contents into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the MATLAB file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extracted data as a DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load MATLAB file into a Python dictionary\n",
    "        matlab_data = sio.loadmat(file_path)\n",
    "    except Exception as e:\n",
    "        # Handle any errors during file loading\n",
    "        print(f\"[ERROR] Failed to load MATLAB file: {file_path}. Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Extract data from MATLAB file and convert it to DataFrame\n",
    "    try:\n",
    "        new_data_frame = pd.DataFrame(matlab_data['sampleMetaOutC'][0][0][-1])\n",
    "    except KeyError as e:\n",
    "        print(f\"[ERROR] Key 'sampleMetaOutC' not found in the MATLAB file: {file_path}. Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Return the extracted DataFrame\n",
    "    return new_data_frame\n",
    "\n",
    "# Main loop for processing directories\n",
    "def process_directories(directory_list):\n",
    "    \"\"\"\n",
    "    Process multiple directories containing MATLAB files, preprocess the data, and save\n",
    "    the results in CSV format.\n",
    "\n",
    "    Args:\n",
    "        directory_list (list): List of directory paths to process.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    explainability_data = pd.DataFrame()\n",
    "\n",
    "    for directory in directory_list:\n",
    "        # Extract patient identifier from the directory name\n",
    "        patient_id = directory[-11:-1]\n",
    "        data_frame = pd.DataFrame()\n",
    "\n",
    "        # Skip processing if the directory is empty\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"[WARNING] Directory does not exist: {directory}\")\n",
    "            continue\n",
    "        if not os.listdir(directory):\n",
    "            print(f\"[INFO] No files found in the directory: {directory}\")\n",
    "            continue\n",
    "\n",
    "        # Process MATLAB files in the directory\n",
    "        for file_path in glob.glob(f\"{directory}/*.mat\"):\n",
    "            print(\"[INFO] Processing MATLAB file: \", file_path)\n",
    "            new_data_frame = load_mat_file(file_path)\n",
    "\n",
    "            # Skip if the DataFrame is empty due to errors\n",
    "            if new_data_frame.empty:\n",
    "                print(f\"[WARNING] Skipping data from file: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            # Append data to the main DataFrame\n",
    "            data_frame = pd.concat([data_frame, new_data_frame], axis=1)\n",
    "\n",
    "        # Preprocess the data\n",
    "        print(\"[INFO] Preprocessing data.\")\n",
    "        try:\n",
    "            processed_data, explained_variance = preprocess(data_frame)\n",
    "        except ValueError as e:\n",
    "            print(f\"[ERROR] Preprocessing failed for directory {directory}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save the preprocessed data\n",
    "        output_path = \"/preprocess_PC/\"\n",
    "        processed_data.to_csv(f\"{output_path}{patient_id}.csv\")\n",
    "\n",
    "        # Save explainability metrics\n",
    "        explainability_data[patient_id] = [explained_variance]\n",
    "        explainability_data.to_csv(\"/explainability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e10f6d-0d03-4068-a844-427ecb32f312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b71704-ac64-4511-b748-cc788fde021a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87bfe2-c367-43e1-8424-c54279ee9f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1603e2-88da-4dda-b598-c4099f029bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
