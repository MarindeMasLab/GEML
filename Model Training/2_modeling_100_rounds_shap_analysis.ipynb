{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "318bec4f-7906-47a4-b7bf-8489e4e2b89f",
   "metadata": {},
   "source": [
    "# Data Analysis and Predictive Modeling with SHAP and XGBoost\n",
    "\n",
    "## Title  \n",
    "**Integrating Machine Learning with Metabolic Models for Precision Trauma Care: Personalized ENDOTYPE Stratification and Metabolic Target Identification**\n",
    "\n",
    "## Authors  \n",
    "- **Igor Marin de Mas** (Copenhagen University Hospital, Rigshospitalet)  \n",
    "- **Lincoln Moura** (Universidade Federal do Ceará)  \n",
    "- **Fernando Luiz Marcelo Antunes** (Universidade Federal do Ceará)  \n",
    "- **Josep Maria Guerrero** (Aalborg University)  \n",
    "- **Pär Ingemar Johansson** (Copenhagen University Hospital, Rigshospitalet)  \n",
    "\n",
    "## Description  \n",
    "This notebook performs a data analysis based on patient features, leveraging machine learning techniques and interpretability tools for classification and pattern identification. The workflow includes:  \n",
    "\n",
    "1. **Data Loading:** Importing preprocessed patient data and explainability variables.  \n",
    "2. **Preprocessing:** Combining individual patient data into a single DataFrame and handling missing values.  \n",
    "3. **Modeling:** Training a classification model using the XGBoost algorithm.  \n",
    "4. **Interpretability:** Utilizing the SHAP library to understand the impact of each feature on the predictive outcomes.  \n",
    "5. **Visualization:** Generating histograms of explained variance and visualizing the top 20 most important features for each group.  \n",
    "6. **Evaluation:** Creating confusion matrices to assess model performance on training and test datasets.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f337c69-45ab-40f7-9a4c-d166a3fc177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import scikitplot as skplt\n",
    "import warnings\n",
    "import shap\n",
    "\n",
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b248546b-1abf-4a81-8f37-4d1c1f9eaeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Utility Functions\n",
    "# -----------------------------\n",
    "\n",
    "def extract_patient_index(file_path):\n",
    "    \"\"\"\n",
    "    Extract the patient index from the file name using regex.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file.\n",
    "\n",
    "    Returns:\n",
    "        int: Patient index extracted from the file name.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the index cannot be extracted from the file name.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    match = re.search(r'(\\d+)', os.path.basename(file_path))\n",
    "    if match:\n",
    "        return int(match.group(1)) - 1\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot extract index from file name: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b79a17-13e2-4c0d-a40e-f83403367a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data(preprocessed_path, num_patients):\n",
    "    \"\"\"\n",
    "    Load preprocessed patient data from CSV files.\n",
    "\n",
    "    Args:\n",
    "        preprocessed_path (str): Path to the directory with preprocessed data.\n",
    "        num_patients (int): Number of patients.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of patient DataFrames indexed by patient number.\n",
    "        list: List of indices for successfully loaded patients.\n",
    "    \"\"\"\n",
    "    patients = [\"patient_\" + str(x) for x in range(num_patients)]\n",
    "    test_indices = []\n",
    "\n",
    "    for file_path in glob.glob(preprocessed_path + '*.csv*'):\n",
    "        try:\n",
    "            index = extract_patient_index(file_path)\n",
    "            print(f\"Index = {index}\")\n",
    "            test_indices.append(index)\n",
    "            patients[index] = pd.read_csv(file_path, index_col=0)\n",
    "        except ValueError as e:\n",
    "            print(f\"[ERROR] {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Could not load file: {file_path}. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return patients, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2fe14-a684-4c69-b9c4-463461e7cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_patient_data(patients, test_indices, target):\n",
    "    \"\"\"\n",
    "    Combine individual patient data into a single DataFrame with target labels.\n",
    "\n",
    "    Args:\n",
    "        patients (list): List of patient DataFrames.\n",
    "        test_indices (list): List of indices for patients.\n",
    "        target (list): Target labels for patients.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with all patient data and target labels.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "\n",
    "    for i in test_indices:\n",
    "        temp = patients[i].T\n",
    "        temp[\"target\"] = target[i]\n",
    "        dataframes.append(temp)\n",
    "\n",
    "    df = pd.concat(dataframes, axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Check for missing values\n",
    "    if df.isna().sum().sum() > 0:\n",
    "        print(\"[WARNING] Missing values detected. Filling with column means.\")\n",
    "        df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830f5fe-24c0-4020-8695-c218b541612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explainability_histogram(explainability):\n",
    "    \"\"\"\n",
    "    Plot a histogram of explained variance from PCA data.\n",
    "\n",
    "    Args:\n",
    "        explainability (pd.DataFrame): DataFrame containing explained variance.\n",
    "    \"\"\"\n",
    "    ax = explainability.hist(figsize=(10, 5))\n",
    "    plt.title(\"\")\n",
    "    plt.xlabel(\"Explained Variance Using PCA with 600 Components (%)\", fontsize=12)\n",
    "    plt.ylabel(\"Number of patients\", fontsize=12)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.gca().set_xticklabels([f'{x:.2%}' for x in plt.gca().get_xticks()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b88b3-a6e8-45f0-b369-afff4a2c4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(clf, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for training and testing predictions.\n",
    "\n",
    "    Args:\n",
    "        clf: Trained classifier.\n",
    "        X_train: Training data.\n",
    "        X_test: Testing data.\n",
    "        y_train: Training labels.\n",
    "        y_test: Testing labels.\n",
    "    \"\"\"\n",
    "    Y_train_pred = clf.predict(X_train)\n",
    "    Y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    skplt.metrics.plot_confusion_matrix(Y_train_pred, y_train, normalize=False, title=\"Confusion Matrix\", cmap=\"Oranges\", ax=ax1)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    skplt.metrics.plot_confusion_matrix(Y_test_pred, y_test, normalize=False, title=\"Confusion Matrix\", cmap=\"Purples\", ax=ax2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a534e005-b30d-4ce4-bc49-a75232e00576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target data\n",
    "target_df = pd.read_csv(\"Patient_Trauma_Groups.csv\", delimiter=\";\")\n",
    "target = target_df[\"Metabo-group\"].values\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf2ffbb-e1ce-42fe-b5ab-f3488a908ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load explainability data\n",
    "explainability = pd.read_csv(\"explainability.csv\", delimiter=\",\", index_col=0).T\n",
    "plot_explainability_histogram(explainability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85162d48-77a8-42fa-9b49-e86872626fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load preprocessed patient data\n",
    "preprocessed_path = \"preprocess_PCA/\"\n",
    "num_patients = 95\n",
    "patients, test_indices = load_preprocessed_data(preprocessed_path, num_patients)\n",
    "\n",
    "# Combine patient data into a single DataFrame\n",
    "df = combine_patient_data(patients, test_indices, target)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99512afd-a49f-4b63-a148-77372bea0be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X, y = df.drop('target', axis=1), df['target'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7f012-cfe1-4b4e-b364-083082088519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "acc = []  # List to store accuracy scores\n",
    "sv_c0, sv_c1, sv_c2, sv_c3 = pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# Run the model training and SHAP analysis 100 times\n",
    "for i in range(100):\n",
    "    print(\"Iteration:\", i)\n",
    "    \n",
    "    # Split features and target variable\n",
    "    X, y = df.drop('target', axis=1), df['target'] - 1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    \n",
    "    # Initialize and train the XGBClassifier\n",
    "    model = XGBClassifier(objective='multi:softmax')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions and compute accuracy\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy_model = accuracy_score(predictions, y_test)\n",
    "    acc.append(accuracy_model)\n",
    "    print(\"Accuracy:\", accuracy_model)\n",
    "    \n",
    "    # Compute SHAP values to explain the model's predictions\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "    \n",
    "    # Aggregate SHAP values by taking the mean absolute value across all samples\n",
    "    aggs = np.abs(shap_values).mean(axis=1)\n",
    "    sv_df = pd.DataFrame(aggs.T, index=X.columns)\n",
    "    \n",
    "    # Define column names dynamically for each class and iteration\n",
    "    col_names = [f\"class{c}_run_{i}\" for c in range(4)]\n",
    "    sv_df.columns = col_names\n",
    "    \n",
    "    # Concatenate SHAP value data for each class across iterations\n",
    "    sv_c0 = pd.concat([sv_df[[col_names[0]]], sv_c0], axis=1, ignore_index=True)\n",
    "    sv_c1 = pd.concat([sv_df[[col_names[1]]], sv_c1], axis=1, ignore_index=True)\n",
    "    sv_c2 = pd.concat([sv_df[[col_names[2]]], sv_c2], axis=1, ignore_index=True)\n",
    "    sv_c3 = pd.concat([sv_df[[col_names[3]]], sv_c3], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67bbb6f-6d10-408f-b543-ee5450d3c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting indices to match MATLAB-style indexing (starting from 1)\n",
    "groups = [sv_c0, sv_c1, sv_c2, sv_c3]\n",
    "for sv in groups:\n",
    "    sv.index += 1  # Increment index by 1\n",
    "    sv[\"mean\"] = sv.iloc[:, :-1].mean(axis=1)  # Compute mean SHAP value per feature\n",
    "\n",
    "# Creating the figure and subplots\n",
    "fig, axes = plt.subplots(figsize=(15, 12), nrows=4, ncols=1, sharex=False, constrained_layout=True)\n",
    "\n",
    "# Titles for each group\n",
    "group_titles = [\n",
    "    \"Top 20 Most Important Features for Group 1 by Average\",\n",
    "    \"Top 20 Most Important Features for Group 2 by Average\",\n",
    "    \"Top 20 Most Important Features for Group 3 by Average\",\n",
    "    \"Top 20 Most Important Features for Group 4 by Average\"\n",
    "]\n",
    "\n",
    "# Generating bar plots for each group\n",
    "for i, (sv, title) in enumerate(zip(groups, group_titles)):\n",
    "    ax = sv.sort_values(by='mean', ascending=False).head(20).plot.bar(ax=axes[i], legend=False)\n",
    "    ax.set_xlabel(title, fontsize=12)  # Set x-axis label\n",
    "    ax.set_ylabel(\"Mean Value for 100 Models\", fontsize=12)  # Set y-axis label\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)  # Adjust tick label size\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ec2bb-0bc0-495d-9809-d94dbfcbcc72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
